{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Algorithm Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This template shows you how to develop a trading algorithm with Quantiacs. \n",
    "* You can run it by clicking **Shift+Enter** for each notebook cell. \n",
    "* You can implement your own algorithm in the **strategy** function definition. \n",
    "* Once you are satisfied with the results you can click the **Submit to the contest** tag. Your system will be evaluated live and take part to the Quantiacs contests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.IPython && (IPython.OutputArea.prototype._should_scroll = function(lines) { return false; })\n",
       "// run this cell for disabling widget scrolling\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "window.IPython && (IPython.OutputArea.prototype._should_scroll = function(lines) { return false; })\n",
    "// run this cell for disabling widget scrolling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This template implements a simple **moving-average crossover** system.\n",
    "\n",
    "It uses the Quantiacs built-in backtester which performs a **multi-pass** simulation.\n",
    "\n",
    "The multi-pass approach isolates each point in time (hence the name multi-pass) and for each point in time it processes the algorithm using only past data. It is slower than a vectorized **single-pass** approach where at any moment the full time series is available to the algorithm. However, the multi-pass approach is more reliable as results are not artificially inflated by unintentional forward-looking on sequential data.\n",
    "\n",
    "Once the following steps are clear, you can implement your own algorithm inside the **strategy** function defined below. \n",
    "\n",
    "Let us first import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **xarray** for data manipulation. Basic informations can be found on our [**documentation**](https://quantiacs.com/documentation/en/user_guide/xarray.html) page or on the xarray [**official**](https://docs.xarray.dev/en/stable/) page.\n",
    "\n",
    "If you prefer to work with [**pandas**](https://pandas.pydata.org/) data structures, simply convert two-dimensional xarrray data structures to pandas ones. \n",
    "\n",
    "Let us consider a concrete example. You start loading Quantiacs data. \n",
    "```python\n",
    "data = qndata.load_ndx_data(tail=365*5)\n",
    "```\n",
    "\n",
    "These are delivered as a three-dimensional data structure indexed by (time, asset, field). Basic fields are the open, close, high and low daily prices. **Close** prices can be selected using:\n",
    "\n",
    "```python\n",
    "close = data.sel(field=\"close\")\n",
    "```\n",
    "\n",
    "The result is a two-dimensional data structure indexed by (time, asset). You can convert this two-dimensional xarray data structure  for the **close price** to a two-dimensional pandas data structure using:\n",
    "\n",
    "```python\n",
    "import pandas\n",
    "close_p = close.to_pandas()\n",
    "```\n",
    "\n",
    "After defining your strategy and computing allocation weights, let us say called weights_p (pandas data structure), you can convert them to an xarray data structure (time, asset) for the final allocation **weights** as follows:\n",
    "\n",
    "```python\n",
    "weights = weights_p.unstack().to_xarray()\n",
    "```\n",
    "\n",
    "Next we import the fundamental modules of the Quantiacs library. You can inspect the source code in the **qnt** directory in your root folder. Or on our [**github**](https://github.com/quantiacs) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        </script>\n",
       "        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import qnt.data as qndata     # functions for loading data\n",
    "import qnt.backtester as qnbt # built-in backtester\n",
    "import qnt.ta as qnta         # technical analysis library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we define the function loading market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(period):\n",
    "    \"\"\"This function loads the data. In the Documentation you will find more details\n",
    "    on the different datasets you can load. You can inspect the source code in your\n",
    "    root folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # loads NASDAQ-100 stock data for the Q18 stock contest:\n",
    "    return qndata.stocks.load_ndx_data(tail = period, dims = (\"time\", \"field\", \"asset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the strategy definition. It must return **allocation weights**: the fraction of capital you want to allocate to each asset on a daily basis. You can take short positions by using negative weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy(data):\n",
    "    \"\"\"This function contains your strategy. It is a simple moving-average crossover system.\n",
    "    It must return allocation weights for all assets for the next trading day (FIXED point in time (note isel(time=-1))) \n",
    "    based on the input xarray \"data\" (data for lookback period).\n",
    "    \"\"\"\n",
    "    \n",
    "    close = data.sel(field=\"close\") #select closing price\n",
    "    \n",
    "    ma_slow = qnta.sma(close, 200).isel(time=-1)\n",
    "    ma_fast = qnta.sma(close, 20).isel(time=-1)\n",
    "    \n",
    "    # keep a long position on the asset when the fast moving average of the price\n",
    "    # is larger than the slow one:\n",
    "    \n",
    "    weights = xr.where(ma_slow < ma_fast, 1, -1) # 1 - long position (positive exposure), \n",
    "                                                 #-1 - short position (negative exposure)\n",
    "    \n",
    "    # this field tags stocks which, at a given point in time,\n",
    "    # are included in the NASDAQ-100 index:\n",
    "    \n",
    "    is_liquid = data.sel(field=\"is_liquid\")\n",
    "\n",
    "    # filter weights so that we are exposed only to stocks which, at a given point in time,\n",
    "    # are or were part of the NASDAQ-100 index, to avoid survivorship bias:\n",
    "    \n",
    "    weights = weights * is_liquid\n",
    "    \n",
    "    # here you can normalize the weights. If the sum of the absolute values for the weights is\n",
    "    # larger than 1, Quantiacs rescale them down uniformly so that the sum of the absolute\n",
    "    # values for the weights is equal to 1.\n",
    "    \n",
    "    weights = weights / 100.0\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final call computes the weights. Check carefully **ERROR** and **WARNINGS**, control plots and if you are happy with the results, click the **Submit to the contest** button!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run last pass...\n",
      "Load data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(367973 of 367973)\u001b[39m |################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(46708 of 46708)\u001b[39m |##################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13140364 of 13140364)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 1/2 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(5515124 of 5515124)\u001b[39m |##############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 2/2 5s\n",
      "Data loaded 5s\n",
      "Run strategy...\n",
      "Load data for cleanup...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(730848 of 730848)\u001b[39m |################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 1/1 3s\n",
      "Data loaded 3s\n",
      "Output cleaning...\n",
      "Fix unique timestamps\n",
      "Forward filling missing prices...\n",
      "Check liquidity...\n",
      "Ok.\n",
      "Check for missed dates...\n",
      "Ok.\n",
      "Normalize and cut big positions for index strategies (nasdaq100, s&p500)...\n",
      "Final normalization...\n",
      "Output cleaning complete.\n",
      "Write result...\n",
      "Write output: /root/fractions.nc.gz\n",
      "---\n",
      "Run first pass...\n",
      "Load data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(46708 of 46708)\u001b[39m |##################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13258480 of 13258480)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 1/2 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(5564696 of 5564696)\u001b[39m |##############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 2/2 3s\n",
      "Data loaded 3s\n",
      "Run strategy...\n",
      "---\n",
      "Load full data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(46708 of 46708)\u001b[39m |##################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170396 of 13170396)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 1/9 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170396 of 13170396)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 2/9 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170396 of 13170396)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 3/9 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170396 of 13170396)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 4/9 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170372 of 13170372)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 5/9 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170304 of 13170304)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 6/9 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170304 of 13170304)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 7/9 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(13170304 of 13170304)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 8/9 6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(7370832 of 7370832)\u001b[39m |##############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 9/9 6s\n",
      "Data loaded 6s\n",
      "---\n",
      "Run iterations...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(5031 of 5031)\u001b[39m |####################| Elapsed Time: 0:01:21 Time:  0:01:210006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations complete.\n",
      "Load data for cleanup and analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(46708 of 46708)\u001b[39m |##################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866284 of 12866284)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 1/9 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866280 of 12866280)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 2/9 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866284 of 12866284)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:000:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 3/9 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866284 of 12866284)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 4/9 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866260 of 12866260)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 5/9 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866196 of 12866196)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 6/9 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866196 of 12866196)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 7/9 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(12866196 of 12866196)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 8/9 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(10531468 of 10531468)\u001b[39m |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched chunk 9/9 6s\n",
      "Data loaded 6s\n",
      "Output cleaning...\n",
      "Fix unique timestamps\n",
      "Forward filling missing prices...\n",
      "Check liquidity...\n",
      "Ok.\n",
      "Check for missed dates...\n",
      "Ok.\n",
      "Normalize and cut big positions for index strategies (nasdaq100, s&p500)...\n",
      "Final normalization...\n",
      "Output cleaning complete.\n",
      "Write result...\n",
      "Write output: /root/fractions.nc.gz\n",
      "---\n",
      "Analyze results...\n",
      "Check...\n",
      "Check liquidity...\n",
      "Ok.\n",
      "Check missed dates...\n",
      "Ok.\n",
      "Check max exposure for index stocks (nasdaq100, s&p500)…\n",
      "Ok.\n",
      "Check the sharpe ratio...\n",
      "Period: 2006-01-01 - 2025-12-31\n",
      "Sharpe Ratio = 0.13018744417863662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR! The Sharpe Ratio is too low. 0.13018744417863662 < 0.7\n",
      "Improve the strategy and make sure that the in-sample Sharpe Ratio more than 0.7.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Align...\n",
      "Calc global stats...\n",
      "---\n",
      "Calc stats per asset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/book/qnt/stats.py:792: FutureWarning:\n",
      "\n",
      "In a future version of xarray the default value for join will change from join='outer' to join='exact'. This change will result in the following ValueError: cannot be aligned with join='exact' because index/labels/sizes are not equal along these coordinates (dimensions): 'time' ('time',) The recommendation is to set join explicitly for this case.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build plots...\n",
      "---\n",
      "Select the asset (or leave blank to display the overall stats):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/book/qnt/stats.py:792: FutureWarning:\n",
      "\n",
      "In a future version of xarray the default value for join will change from join='outer' to join='exact'. This change will result in the following ValueError: cannot be aligned with join='exact' because index/labels/sizes are not equal along these coordinates (dimensions): 'time' ('time',) The recommendation is to set join explicitly for this case.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d32d5c56b0463cb5580fe71ca0f7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Combobox(value='', description='asset', options=('', 'NAS:AAL', 'NAS:AAPL', 'NAS:ABNB', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here you can run the backtester which evaluates the performance of your strategy since start_date\n",
    "# lookback_period is expressed in calendar days and it expresses the length\n",
    "# of the rolling window used by the backtester. It must be large enough\n",
    "# to include all indicators for your strategy (expressed in trading days).\n",
    "# The smaller the window, the more efficient the evaluation.\n",
    "\n",
    "weights = qnbt.backtest(\n",
    "    competition_type = \"stocks_nasdaq100\",\n",
    "    load_data        = load_data,\n",
    "    lookback_period  = 365*4,\n",
    "    start_date       = \"2006-01-01\",\n",
    "    strategy         = strategy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For improving speed you can implement your strategy using a **single-pass approach**. Your algorithm must return arrays for the allocation weights. In this implementation the full time series is accessible to your algo at any point in time, so you should make sure that no forward looking is taking place and your algo does not use future information for predicting the past. \n",
    "\n",
    "**IMPORTANT: Any** implementation in your Notebook (single-pass or multi-pass) will be processed **after submission on our servers** using a multi-pass approach in order to prevent forward looking. \n",
    "\n",
    "If the **Sharpe ratio** of your submission does not match your expectations from your Notebook research, please review your implementation: most likely some forward looking is taking place, for example by computing some global mean value which is then used for taking trading decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Single-pass implementation (for prototyping)\n",
    "import xarray as xr\n",
    "\n",
    "import qnt.data as qndata\n",
    "import qnt.output as qnout\n",
    "import qnt.ta as qnta\n",
    "import qnt.stats as qns\n",
    "\n",
    "# load data:\n",
    "data = qndata.stocks.load_ndx_data(min_date=\"2006-01-01\")\n",
    "\n",
    "# calculate weights:\n",
    "close = data.sel(field=\"close\")\n",
    "ma_slow = qnta.sma(close, 200)\n",
    "ma_fast = qnta.sma(close, 20)\n",
    "weights = xr.where(ma_fast > ma_slow, 1, -1)\n",
    "\n",
    "# liquidity filter:\n",
    "is_liquid = data.sel(field=\"is_liquid\")\n",
    "\n",
    "# set weights:\n",
    "weights = weights * is_liquid\n",
    "weights = weights / 100.0\n",
    "\n",
    "# clean weights taking corner cases into account:\n",
    "weights = qnout.clean(weights, data, \"stocks_nasdaq100\")\n",
    "\n",
    "# check before submission:\n",
    "qnout.check(weights, data, \"stocks_nasdaq100\")\n",
    "\n",
    "# write results:\n",
    "qnout.write(weights)\n",
    "\n",
    "# calculate statistics for checking:\n",
    "stats = qns.calc_stat(data, weights.sel(time=slice(\"2006-01-01\",None)))\n",
    "stats.to_pandas().tail()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
